# Structured output formatting

## Overview

Structured output formatting is a method for ensuring that responses generated by Large Language Models (LLMs) adhere to a predefined structure, often using JSON or similar schemas. This approach is critical in scenarios where LLM outputs need to be parsed, validated, or consumed programmatically, such as in tool calling, API responses, or function executions. By defining and enforcing a schema, developers can reliably extract information, reduce ambiguity, and enable seamless integration with downstream systems.

---

## Explanation

LLMs by default generate free-form text, which can be ambiguous or difficult to parse. Structured output formatting addresses this by:

- **Defining a function or tool with a JSON schema**: You specify the expected input and output structure, often using JSON Schema.
- **Registering the function with the LLM client**: The LLM is informed about the available functions/tools and their input formats.
- **Prompting the LLM for structured outputs**: The LLM is instructed to respond in a way that matches the schema, enabling automatic extraction and execution of function calls.
- **Handling tool call and function call messages**: The system interprets special messages (often in OpenAI or similar APIs) that indicate when a function/tool should be called, and with what arguments.

This process allows the LLM to act as an orchestrator, calling backend tools or functions as necessary and ensuring that all communication follows a well-defined, machine-readable structure.

---

## Examples

### 1. Defining and Registering a Function with JSON Schema

Suppose you want the LLM to call a function `get_weather` that takes a `city` argument:

```python
import openai

get_weather_schema = {
    "name": "get_weather",
    "description": "Get the current weather for a specified city.",
    "parameters": {
        "type": "object",
        "properties": {
            "city": {
                "type": "string",
                "description": "Name of the city to get weather for."
            }
        },
        "required": ["city"]
    }
}

# Register the function with your LLM client (example: OpenAI)
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "What's the weather in Paris?"}],
    functions=[get_weather_schema]
)
```

### 2. LLM Responding with a Structured Function Call

The LLM's response might include a function call message:

```json
{
  "role": "assistant",
  "content": null,
  "function_call": {
    "name": "get_weather",
    "arguments": "{ \"city\": \"Paris\" }"
  }
}
```

Your application would then:

- Parse the `function_call` object.
- Call your backend `get_weather("Paris")`.
- Return the result to the user.

### 3. Configuring Structured Output Formatting in Prompting

When you want the LLM to always respond in a specific JSON format:

```python
prompt = """
Extract the following information from the text and respond in JSON:
- name
- email
- phone number

Text: "Contact Jane Doe at jane.doe@example.com or call 555-1234."
"""

# Use function-calling or output parser libraries (e.g., Pydantic, Cerberus) to validate the output.
```

Expected output:

```json
{
  "name": "Jane Doe",
  "email": "jane.doe@example.com",
  "phone": "555-1234"
}
```

---

## Best Practices

- **Define clear, concise schemas**: Use JSON Schema or similar to describe expected arguments and outputs.
- **Validate inputs and outputs**: Always validate the structure before processing to prevent errors.
- **Handle edge cases**: Consider what happens if the LLM returns malformed or incomplete data.
- **Use incremental deployment**: Start with simple schemas and expand as you gain confidence.
- **Log and monitor**: Track schema mismatches and failures for debugging and improvement.
- **Keep functions atomic**: Each function should have a single, clear responsibility.

**Common pitfalls:**

- Overly complex schemas may confuse the LLM or lead to parsing errors.
- Free-form text in outputs can break downstream parsing.
- Failing to handle missing or extra fields gracefully.

---

## Related Concepts

- [Function Calling with LLMs](https://platform.openai.com/docs/guides/function-calling)
- [Tool Use and Orchestration](#)
- [Dependency Injection with LLM Tooling](#)
- [Prompt Engineering for Structured Outputs](#)
- [JSON Schema Documentation](https://json-schema.org/)

---

**Further Reading:**

- [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)
- [LangChain Output Parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/)
- [Pydantic Documentation](https://docs.pydantic.dev/)
